{
  "2312.03788v1": {
    "title": "SmoothQuant+: Accurate and Efficient 4-bit Post-Training WeightQuantization for LLM",
    "authors": [
      "Jiayi Pan",
      "Chengcan Wang",
      "Kaifu Zheng",
      "Yangguang Li",
      "Zhenyu Wang",
      "Bin Feng"
    ],
    "summary": "Large language models (LLMs) have shown remarkable capabilities in various\ntasks. However their huge model size and the consequent demand for\ncomputational and memory resources also pose challenges to model deployment.\nCurrently, 4-bit post-training quantization (PTQ) has achieved some success in\nLLMs, reducing the memory footprint by approximately 75% compared to FP16\nmodels, albeit with some accuracy loss. In this paper, we propose SmoothQuant+,\nan accurate and efficient 4-bit weight-only PTQ that requires no additional\ntraining, which enables lossless in accuracy for LLMs for the first time. Based\non the fact that the loss of weight quantization is amplified by the activation\noutliers, SmoothQuant+ smoothes the activation outliers by channel before\nquantization, while adjusting the corresponding weights for mathematical\nequivalence, and then performs group-wise 4-bit weight quantization for linear\nlayers. We have integrated SmoothQuant+ into the vLLM framework, an advanced\nhigh-throughput inference engine specially developed for LLMs, and equipped it\nwith an efficient W4A16 CUDA kernels, so that vLLM can seamlessly support\nSmoothQuant+ 4-bit weight quantization. Our results show that, with\nSmoothQuant+, the Code Llama-34B model can be quantized and deployed on a A100\n40GB GPU, achieving lossless accuracy and a throughput increase of 1.9 to 4.0\ntimes compared to the FP16 model deployed on two A100 40GB GPUs. Moreover, the\nlatency per token is only 68% of the FP16 model deployed on two A100 40GB GPUs.\nThis is the state-of-the-art 4-bit weight quantization for LLMs as we know.",
    "pdf_url": "http://arxiv.org/pdf/2312.03788v1",
    "published": "2023-12-06"
  },
  "2504.18415v1": {
    "title": "BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs",
    "authors": [
      "Hongyu Wang",
      "Shuming Ma",
      "Furu Wei"
    ],
    "summary": "Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by\nactivation outliers, which complicate quantization to low bit-widths. We\nintroduce BitNet v2, a novel framework enabling native 4-bit activation\nquantization for 1-bit LLMs. To tackle outliers in attention and feed-forward\nnetwork activations, we propose H-BitLinear, a module applying an online\nHadamard transformation prior to activation quantization. This transformation\nsmooths sharp activation distributions into more Gaussian-like forms, suitable\nfor low-bit representation. Experiments show BitNet v2 trained from scratch\nwith 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2\nachieves minimal performance degradation when trained with native 4-bit\nactivations, significantly reducing memory footprint and computational cost for\nbatched inference.",
    "pdf_url": "http://arxiv.org/pdf/2504.18415v1",
    "published": "2025-04-25"
  },
  "2105.03536v1": {
    "title": "Pareto-Optimal Quantized ResNet Is Mostly 4-bit",
    "authors": [
      "AmirAli Abdolrashidi",
      "Lisa Wang",
      "Shivani Agrawal",
      "Jonathan Malmaud",
      "Oleg Rybakov",
      "Chas Leichner",
      "Lukasz Lew"
    ],
    "summary": "Quantization has become a popular technique to compress neural networks and\nreduce compute cost, but most prior work focuses on studying quantization\nwithout changing the network size. Many real-world applications of neural\nnetworks have compute cost and memory budgets, which can be traded off with\nmodel quality by changing the number of parameters. In this work, we use ResNet\nas a case study to systematically investigate the effects of quantization on\ninference compute cost-quality tradeoff curves. Our results suggest that for\neach bfloat16 ResNet model, there are quantized models with lower cost and\nhigher accuracy; in other words, the bfloat16 compute cost-quality tradeoff\ncurve is Pareto-dominated by the 4-bit and 8-bit curves, with models primarily\nquantized to 4-bit yielding the best Pareto curve. Furthermore, we achieve\nstate-of-the-art results on ImageNet for 4-bit ResNet-50 with\nquantization-aware training, obtaining a top-1 eval accuracy of 77.09%. We\ndemonstrate the regularizing effect of quantization by measuring the\ngeneralization gap. The quantization method we used is optimized for\npracticality: It requires little tuning and is designed with hardware\ncapabilities in mind. Our work motivates further research into optimal numeric\nformats for quantization, as well as the development of machine learning\naccelerators supporting these formats. As part of this work, we contribute a\nquantization library written in JAX, which is open-sourced at\nhttps://github.com/google-research/google-research/tree/master/aqt.",
    "pdf_url": "http://arxiv.org/pdf/2105.03536v1",
    "published": "2021-05-07"
  },
  "2405.18144v3": {
    "title": "4-bit Shampoo for Memory-Efficient Network Training",
    "authors": [
      "Sike Wang",
      "Pan Zhou",
      "Jia Li",
      "Hua Huang"
    ],
    "summary": "Second-order optimizers, maintaining a matrix termed a preconditioner, are\nsuperior to first-order optimizers in both theory and practice. The states\nforming the preconditioner and its inverse root restrict the maximum size of\nmodels trained by second-order optimizers. To address this, compressing 32-bit\noptimizer states to lower bitwidths has shown promise in reducing memory usage.\nHowever, current approaches only pertain to first-order optimizers. In this\npaper, we propose the first 4-bit second-order optimizers, exemplified by 4-bit\nShampoo, maintaining performance similar to that of 32-bit ones. We show that\nquantizing the eigenvector matrix of the preconditioner in 4-bit Shampoo is\nremarkably better than quantizing the preconditioner itself both theoretically\nand experimentally. By rectifying the orthogonality of the quantized\neigenvector matrix, we enhance the approximation of the preconditioner's\neigenvector matrix, which also benefits the computation of its inverse 4-th\nroot. Besides, we find that linear square quantization slightly outperforms\ndynamic tree quantization when quantizing second-order optimizer states.\nEvaluation on various networks for image classification and natural language\nmodeling demonstrates that our 4-bit Shampoo achieves comparable performance to\nits 32-bit counterpart while being more memory-efficient.",
    "pdf_url": "http://arxiv.org/pdf/2405.18144v3",
    "published": "2024-05-28"
  },
  "2411.04965v1": {
    "title": "BitNet a4.8: 4-bit Activations for 1-bit LLMs",
    "authors": [
      "Hongyu Wang",
      "Shuming Ma",
      "Furu Wei"
    ],
    "summary": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference.",
    "pdf_url": "http://arxiv.org/pdf/2411.04965v1",
    "published": "2024-11-07"
  }
}